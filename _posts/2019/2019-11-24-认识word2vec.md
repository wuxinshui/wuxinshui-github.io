---
layout: post
title: 认识word2vec
category: NLP-自然语言处理 
tags: [NLP-自然语言处理]
---

>Tool for computing continuous distributed representations of words.
 

## 论文

[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)

[Natural Language Processing (almost) from Scratch](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35671.pdf)

[Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781)

[word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738)


## 官网

[word2vec](https://code.google.com/archive/p/word2vec/)

## API

[models.word2vec – Word2vec embeddings](https://radimrehurek.com/gensim/models/word2vec.html)


## 语料


[搜狗实验室](http://www.sogou.com/labs/resource/list_news.php)

[Pre-trained word vectors of 30+ languages](https://github.com/Kyubyong/wordvectors)

中文维基分词语料：链接 https://pan.baidu.com/s/1qXKIPp6 密码 kade

[腾讯AI Lab开源大规模高质量中文词向量数据，800万中文词随你用](https://mp.weixin.qq.com/s/b9NWR0F7GQLYtgGSL50gQw)

## 实战

```python
# 加载包
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

import logging
import itertools
import gensim
from gensim import utils


```


```python
# 训练模型
sentences = LineSentence('wiki.zh.word-utf8.text')
# min_count指定了需要训练词语的最小出现次数，默认为5
# size指定了训练时词向量维度，默认为100
# worker指定了完成训练过程的线程数，默认为1不使用多线程。只有注意安装Cython的前提下该参数设置才有意义
#sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。
model = Word2Vec(sentences, size=128, window=5, min_count=5, workers=4,sg = 1)
```


```python
# 保存模型

#保存的文件不能利用文本编辑器查看但是保存了训练的全部信息，可以在读取后追加训练
model.save('wiki_zh_word_embedding_128_again.m')
#保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练
model.wv.save_word2vec_format('wiki_zh_word_embedding_128_sg.m', binary=False)
```


```python
# 加载模型
#model = Word2Vec.load("wiki_zh_word_embedding_128.m")
model=gensim.models.Word2Vec.load("wiki_zh_word_embedding_128_sg.m")
```



```python
# 使用模型
items = model.wv.most_similar(u'中国')
print(len(items))
#model.wv.most_similar(u'男人',  u'女人')
for i, item in enumerate(items):
	print(i, item[0], item[1])
```

    10
    0 大陆 0.7746931314468384
    1 我国 0.6922389268875122
    2 经营报 0.6868888139724731
    3 北京 0.6853708624839783
    4 内地 0.6779896020889282
    5 江苏网 0.6692827343940735
    6 军网 0.6671650409698486
    7 亚洲各国 0.6626995801925659
    8 欧中 0.6616818904876709
    9 东盟自由贸易区 0.6579015851020813

```python
# 使用模型
# 使用模型
#items = model.wv.most_similar(u'男人',  u'女人')

items = model.wv.most_similar(positive=['国王', '男人'], negative=['王后'])
for i, item in enumerate(items):
	print(i, item[0], item[1])
```

    0 女人 0.6733542680740356
    1 胆小鬼 0.6426182985305786
    2 家伙 0.6325250864028931
    3 重色轻友 0.6242005228996277
    4 事来 0.6211745142936707
    5 很胖 0.6184269785881042
    6 瘦瘦的 0.613095760345459
    7 男孩 0.611844003200531
    8 浑蛋 0.6114282608032227
    9 少来 0.6114280819892883


```python
from sklearn.decomposition import PCA
from matplotlib import pyplot
%matplotlib inline


X = model[model.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# 可视化展示
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words):
	pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
```

      

![](https://oscimg.oschina.net/oscnet/3b7497cac1f309f9aa84da283451661dfe4.png)




